{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实现功能：\n",
    "采用两个数据集进行文章相似度推荐，第一个数据集为搜狗数据集，第二个数据集为本次业务数据集。数据集位于 dataset文件夹，因为系统问题上传失败。  \n",
    "最后只需要运行 fun1, func2函数，func1实现对原始数据进行预处理，把处理后的中间文件存放在本地，服务器对该函数只需要间隔几个小时运行一次即可。func2读取func1的结果，对新传入的文章进行相似度比较，返回相似度较最高的几篇文章推荐，推荐的文章位于func1的返回文件中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 加载package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import re\n",
    "import jieba\n",
    "import jieba.posseg as pseg\n",
    "import jieba.analyse\n",
    "import os\n",
    "import random\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from concurrent import futures\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "import pandas as pd\n",
    "import dill\n",
    "import pickle\n",
    "\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# 预先定义停用词路径\n",
    "stop_filepath = \"./chinese_stop_words.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 预定义函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def load_stopwords(stop_filepath):\n",
    "    '''\n",
    "    根据停用词文件所在的路径加载停用词，返回停用词列表\n",
    "    \n",
    "    参数\n",
    "    ----------\n",
    "    stop_filepath : string\n",
    "        停用词文件所在的路径\n",
    "        \n",
    "    返回\n",
    "    stopwords : list\n",
    "        储存停用词\n",
    "    ----------\n",
    "    \n",
    "    '''\n",
    "#     stop_filepath = \"/Users/cyy7645/Documents/internship/article_recom/chinese_stop_words.txt\"\n",
    "    stopwords = [line.strip() for line in \n",
    "                 open(stop_filepath, 'r', encoding='utf-8').readlines()]\n",
    "    \n",
    "    return stopwords\n",
    "\n",
    "\n",
    "def participle(article, cut_all = False, cut_for_search = False):\n",
    "    '''\n",
    "    对文章进行分词，可选择分词方法\n",
    "    \n",
    "    参数\n",
    "    ----------\n",
    "    article : string\n",
    "        代表整片文章的字符串\n",
    "    cut_all : bool 默认 False\n",
    "        当值为True时，使用全模式分词\n",
    "    cut_for_search : bool 默认 False\n",
    "        当值为True时，使用搜索引擎模式\n",
    "        \n",
    "    返回\n",
    "    ----------\n",
    "    article : string\n",
    "        分完词的文章，词和词间隔为空格\n",
    "    '''\n",
    "    if jieba.cut_for_search == True:\n",
    "        # 搜索引擎模式\n",
    "        article = ' '.join(jieba.cut_for_search(article))\n",
    "    elif cut_all == False:\n",
    "        # 使用精准模式分词\n",
    "        article = ' '.join(jieba.cut(article))\n",
    "    else:\n",
    "        # 使用全模式分词\n",
    "        article = ' '.join(jieba.cut(article, cut_all=True))\n",
    "        \n",
    "    return article\n",
    "\n",
    "\n",
    "def clear_stopwords(article, stopwords):\n",
    "    '''\n",
    "    对分词完成的每篇文章去除停用词\n",
    "    \n",
    "    参数\n",
    "    ----------\n",
    "    article : string\n",
    "        分词后的文章，词和词间隔为空格\n",
    "    stopwords : list\n",
    "        储存停用词的列表\n",
    "    \n",
    "    返回\n",
    "    ----------\n",
    "    article : string\n",
    "        删除通用词后的文章，词和词间隔为空格\n",
    "    '''\n",
    "    article = ' '.join(word for word in article.split() if word not in stopwords)\n",
    "    \n",
    "    return article\n",
    "\n",
    "        \n",
    "def divide_words(contexts,stopwords, cut_all = False, cut_for_search = False):\n",
    "    '''\n",
    "    对文章整个文章集(包含某一类的所有文章)进行分词\n",
    "    \n",
    "    参数\n",
    "    ----------\n",
    "    contexts : list\n",
    "        包含整个文章集的列表，每个元素是一篇文章\n",
    "    stopwords : list\n",
    "        包含停用词的列表\n",
    "    cut_all : bool 默认 False\n",
    "        当值为True时，使用全模式分词\n",
    "    cut_for_search : bool 默认 False\n",
    "        当值为True时，使用搜索引擎模式\n",
    "        \n",
    "    返回\n",
    "    ----------\n",
    "    articles_list : list\n",
    "        包含所有唯一文章的列表，其中的文章已经分词，去除停用词\n",
    "        \n",
    "    '''\n",
    "#     # 每篇文章都是以 (责任编辑:马莹莹) 结尾，基于这个原理提取出文章存入list\n",
    "#     articles_list = articles_in_category.strip().split(')\\n')\n",
    "    # 对list中的文章去重\n",
    "    for i in range(len(contexts)):\n",
    "        contexts[i] = participle(contexts[i], cut_all = False, \\\n",
    "                                      cut_for_search = False)\n",
    "        contexts[i] = clear_stopwords(contexts[i],stopwords)\n",
    "        \n",
    "    # 返回经过分词 去除停用词后的文章集\n",
    "    return contexts\n",
    "\n",
    "\n",
    "def preprocess_categories(dir_path, tar_path, stopwords, cut_all = False, \\\n",
    "                          cut_for_search = False):\n",
    "    '''\n",
    "    用单线程对原始数据进行 分词、去除停用词、去重后 储存在本地 \n",
    "    注意！ 针对搜狗数据集的预处理\n",
    "    \n",
    "    参数\n",
    "    ----------\n",
    "    dir_path : string\n",
    "        数据集所在的路径 e.g. \"/Users/cyy7645/Desktop/contents/*.txt\"\n",
    "    tar_path : string\n",
    "        预处理完成后数据所存储的路径 e.g. \"/divided_context_no_sim/\"\n",
    "        \n",
    "    返回\n",
    "    ----------\n",
    "    无\n",
    "    '''\n",
    "    data_paths = glob.glob(dir_path)\n",
    "    # data_paths为一个list，包含所有文件的路径\n",
    "    for path in data_paths:\n",
    "        with open(path,'r') as fp:\n",
    "            articles_in_category = fp.read()\n",
    "            # 对文章集分词\n",
    "            div_articles_in_category = divide_words(articles_in_category,stopwords, \\\n",
    "                                                    cut_all = False, cut_for_search = False)\n",
    "            # 划分成一篇篇文章\n",
    "            div_articles_in_category = '\\n'.join(div_articles_in_category)\n",
    "            # 要写入本地的路径\n",
    "            write_path = '/'.join(path.split('/')[:-2])+'tar_path'+path.split('/')[-1]\n",
    "            print(write_path)\n",
    "            with open(write_path, \"w\") as write_file:\n",
    "            # 把分词好的文章写回到文件中\n",
    "                write_file.write(div_articles_in_category)\n",
    "    print(\"proprocess done...\")\n",
    "    \n",
    "\n",
    "\n",
    "# 从每篇文章中提取关键词\n",
    "def extract_keywords(article, topK=20, withWeight=True, allowPOS=()):\n",
    "    '''\n",
    "    根据单词在文章中出现的频率从每篇文章中提取关键词，返回一个包含关键词的列表和包含主体对应权重的列表\n",
    "    \n",
    "    参数\n",
    "    ----------\n",
    "    article: string\n",
    "        表示文章的字符串\n",
    "    topK : int 默认 20\n",
    "        返回几个 TF/IDF 权重最大的关键词\n",
    "    withWeight : bool 默认 True    \n",
    "        是否一并返回关键词权重值\n",
    "    allowPOS : tuple 默认为空\n",
    "        仅包括指定词性的词\n",
    "        \n",
    "    返回\n",
    "    ----------\n",
    "    items : list \n",
    "        包含关键词的列表\n",
    "    weights : list\n",
    "        关键词对应的权重\n",
    "    '''\n",
    "    keywords = jieba.analyse.extract_tags(article, topK=20, withWeight=True, allowPOS=())\n",
    "    items = []\n",
    "    weights = []\n",
    "    for item in keywords:\n",
    "    # 分别为关键词和相应的权重\n",
    "        items.append(item[0])\n",
    "        weights.append(item[1])\n",
    "        \n",
    "    return items, weights\n",
    "\n",
    "\n",
    "def get_dict_and_list_of_articles(path):\n",
    "    '''\n",
    "    根据预处理完数据所在的路径，生成储存文章及其所属类别的 list和dict\n",
    "    \n",
    "    参数\n",
    "    ----------\n",
    "    divided_paths : string\n",
    "        预处理完成的数据所在的路径，e.g. divided_paths = \n",
    "                    glob.glob('/Users/cyy7645/Documents/internship/data/cleared_news/*.txt')\n",
    "    \n",
    "    返回\n",
    "    ----------\n",
    "    articles_str : list\n",
    "        储存整个数据集的文章\n",
    "    articles_classes : list\n",
    "        储存整个数据集的文章所对应的类[int]\n",
    "    ids_articles : dict\n",
    "        储存文章和其类别一一对应的字典\n",
    "        value为文章，key为文章对应的类 int\n",
    "    ids_classes_articles : dict\n",
    "        储存文章id和其所属类别的字典\n",
    "        key为文章的id（从1开始递增），value为该文章所属的类别\n",
    "    \n",
    "    '''\n",
    "    divided_paths = glob.glob(path)\n",
    "    articles_str = []\n",
    "    ids_articles = {}\n",
    "    articles_classes = []\n",
    "    ids_classes_articles = {}\n",
    "    count_articles = 1\n",
    "    article_class = 1\n",
    "    label_classes = {}\n",
    "    for path in divided_paths:\n",
    "        articles_dict = {}\n",
    "        with open (path, 'r') as p:\n",
    "            articles = p.read().split(\"\\n\")\n",
    "            label = path.split('/')[-1].split('.')[0]\n",
    "            for article in articles:\n",
    "                articles_str.append(article)\n",
    "                ids_articles[count_articles] = article\n",
    "                articles_classes.append(article_class)\n",
    "                ids_classes_articles[count_articles] = article_class\n",
    "                count_articles += 1\n",
    "        article_class += 1\n",
    "        label_classes[label] = article_class\n",
    "        \n",
    "    return articles_str, articles_classes, ids_articles, ids_classes_articles, label_classes\n",
    "\n",
    "def get_dict_and_list_of_items(path):\n",
    "    '''\n",
    "    根据预处理完数据所在的路径，生成储存关键字及其所属类别的 list和dict\n",
    "    \n",
    "    参数\n",
    "    ----------\n",
    "    divided_paths : string\n",
    "        预处理完成的数据所在的路径，e.g. divided_paths = \n",
    "                    glob.glob('/Users/cyy7645/Documents/internship/data/cleared_news/*.txt')\n",
    "    \n",
    "    返回\n",
    "    ----------\n",
    "    items_str : list\n",
    "        储存整个数据集的文章对应的关键字\n",
    "    items_classes : list\n",
    "        储存整个数据集的文章对应的关键字所对应的类[int]\n",
    "    ids_items : dict\n",
    "        储存文章对应的关键字和其类别一一对应的字典\n",
    "        value为文章对应的关键字，key为文章对应的类 int\n",
    "    ids_classes : dict\n",
    "        储存文章id和其所属类别的字典\n",
    "        key为文章的id（从1开始递增），value为该文章所属的类别\n",
    "    label_classes : dict\n",
    "        储存标签和id的字典\n",
    "        key为标签，value为id\n",
    "    \n",
    "    '''\n",
    "    divided_paths = glob.glob(path)\n",
    "    items_str = []\n",
    "    ids_items = {}\n",
    "    items_classes = []\n",
    "    ids_classes = {}\n",
    "    label_classes = {}\n",
    "    count_articles = 1\n",
    "    article_class = 1\n",
    "    for path in divided_paths:\n",
    "        articles_dict = {}\n",
    "        with open (path, 'r') as p:\n",
    "            articles = p.read().split(\"\\n\")\n",
    "            label = path.split('/')[-1].split('.')[0]\n",
    "            for article in articles:\n",
    "                items,weights = extract_keywords(article, topK=20, \n",
    "                                                 withWeight=True, allowPOS=())\n",
    "                items_str.append(items)\n",
    "                ids_items[count_articles] = article\n",
    "                items_classes.append(article_class)\n",
    "                ids_classes[count_articles] = article_class\n",
    "                count_articles += 1\n",
    "        label_classes[label] = article_class\n",
    "        article_class += 1\n",
    "        \n",
    "    return items_str, items_classes, ids_items, ids_classes, label_classes\n",
    "\n",
    "\n",
    "def shuffle_list(articles_str, articles_classes):\n",
    "    '''\n",
    "    打乱文章的顺序，同时保证articles_str和articles_classes一一对应\n",
    "    \n",
    "    参数\n",
    "    ----------\n",
    "    articles_str : list\n",
    "        储存整个数据集的文章\n",
    "        \n",
    "    articles_classes : list\n",
    "        储存整个数据集的文章所对应的类[int]\n",
    "        \n",
    "    返回\n",
    "    ----------\n",
    "    articles_str : list\n",
    "        打乱顺序后整个数据集的文章\n",
    "        \n",
    "    articles_classes : list\n",
    "        打乱顺序后整个数据集的文章所对应的类[int]\n",
    "    '''\n",
    "    combined = list(zip(articles_str, articles_classes))\n",
    "    random.shuffle(combined)\n",
    "    articles_str[:], articles_classes[:] = zip(*combined)\n",
    "    \n",
    "    return articles_str, articles_classes\n",
    "\n",
    "\n",
    "def split_vectors_and_classes(articles_str, articles_classes):\n",
    "    '''\n",
    "    生成TF-IDF表，并把储存文章和类别的列表划分训练集和测试集\n",
    "    \n",
    "    参数\n",
    "    ----------\n",
    "    articles_str : list\n",
    "        打乱顺序后整个数据集的文章(关键词)\n",
    "        articles_str可能是一维数组（文章），也可能是二维数组（关键词）\n",
    "    articles_classes : list\n",
    "        打乱顺序后整个数据集的文章所对应的类[int]\n",
    "        \n",
    "    返回\n",
    "    ----------\n",
    "    vectors : list\n",
    "        IF-IDF表\n",
    "    train_vectors : list\n",
    "        训练集的IF-IDF表\n",
    "    test_vectors : list\n",
    "        测试集的IF-IDF表\n",
    "    articles_classes_test_np : numpy\n",
    "        测试集文章所对应的类别\n",
    "    articles_classes_train_np : numpy\n",
    "        训练集文章所对应的类别\n",
    "    vectorizer : class\n",
    "        文本特征提取器    \n",
    "    '''\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    if isinstance(articles_str[0], list) == False:\n",
    "        vectors = vectorizer.fit_transform(articles_str)\n",
    "    else:\n",
    "        articles_str = [' '.join(x) for x in articles_str]\n",
    "        vectors = vectorizer.fit_transform(articles_str)\n",
    "    train_vectors = vectors[: int(9 * vectors.shape[0]/10), :]\n",
    "    test_vectors = vectors[int(-vectors.shape[0]/10): , :]\n",
    "    articles_classes_test = articles_classes[int(-len(articles_classes)/10):]\n",
    "    articles_classes_train = articles_classes[:int(9 * len(articles_classes)/10)]\n",
    "    articles_classes_test_np = np.array(articles_classes_test)\n",
    "    articles_classes_train_np = np.array(articles_classes_train)\n",
    "    \n",
    "    # 返回训练/测试TF-IDF矩阵  训练/测试类别(numpy类型) vectorizer\n",
    "    return train_vectors, test_vectors, articles_classes_test_np, \\\n",
    "            articles_classes_train_np, vectorizer, vectors\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_categories_muti(data_paths, tar_path, stopwords, \n",
    "                               cut_all = False, cut_for_search = False):\n",
    "    '''\n",
    "    使用多线程对原始数据进行 分词、去除停用词、去重后 储存在本地\n",
    "    参数详情见 preprocess_categories 函数\n",
    "    注意！ 针对搜狗数据集的预处理\n",
    "    '''\n",
    "    def fun(path):\n",
    "        with open(path,'r') as fp:\n",
    "            articles_in_category = fp.read()\n",
    "            div_articles_in_category = divide_words(articles_in_category, stopwords, \n",
    "                                                    cut_all = False, cut_for_search = False)\n",
    "            div_articles_in_category = '\\n'.join(div_articles_in_category)\n",
    "            write_path = '/'.join(path.split('/')[:-2])+'/divided_context_no_sim/'+ \\\n",
    "                            path.split('/')[-1]\n",
    "            print(write_path)\n",
    "            with open(write_path, \"w\") as write_file:\n",
    "            # 把分词好的文章写回到文件中\n",
    "                write_file.write(div_articles_in_category)\n",
    "\n",
    "    with futures.ThreadPoolExecutor(20) as executor:\n",
    "        res = executor.map(fun, data_paths)\n",
    "    print(\"preprocess done...\")\n",
    "\n",
    "def get_contents_from_csv(path, days):\n",
    "    '''\n",
    "    从.csv文件中提取文章的id、创建日期、标题、描述和内容\n",
    "    \n",
    "    参数\n",
    "    ----------\n",
    "    path : string\n",
    "        .csv文件所在的路径\n",
    "    \n",
    "    返回\n",
    "    ----------\n",
    "    id : list\n",
    "        存放文章对应的id\n",
    "    times : list\n",
    "        存放某一类别下文章的创建日期 只保留年月日\n",
    "    titles : list\n",
    "        存放某一类别下所有文章的标题\n",
    "    descriptions : list\n",
    "        存放某一类别下所有文章的描述\n",
    "    contents : list\n",
    "        存放某一类别下所有文章的内容\n",
    "    '''\n",
    "    fields = ['id','create_time','title', 'description','content']\n",
    "    df = pd.read_csv(path, usecols=fields)\n",
    "    \n",
    "    df['create_time'] = pd.to_datetime(df['create_time'])\n",
    "    latest_date = df['create_time'].max()\n",
    "    earlist_date = latest_date-np.timedelta64(days,'D')\n",
    "    df = df[(df['create_time'] > earlist_date) & (df['create_time'] <= latest_date)]\n",
    "    \n",
    "    df = df.drop_duplicates(subset=['description'], keep=False)\n",
    "    ids = df['id'].tolist()\n",
    "    times = df['create_time'].tolist()\n",
    "#     times = [x.split()[0] for x in times]\n",
    "    titles = df['title'].tolist()\n",
    "    descriptions = df['description'].tolist()\n",
    "    original_contents = df['content'].tolist()\n",
    "    # 根据文件的内容储存方式使用正则表达式提取内容\n",
    "    symbol = re.compile(r'\\<.*?\\>')\n",
    "    contents = []\n",
    "    for content in original_contents:\n",
    "        cleared_content = symbol.sub('',content)\n",
    "        cleared_content = ''.join(cleared_content.split('\\n'))\n",
    "        contents.append(cleared_content)\n",
    "    return ids, times, titles, descriptions, contents\n",
    "\n",
    "def combine_title_desc_content(titles, descriptions, contents):\n",
    "    '''\n",
    "    把文章的标题、描述和内容合并成一个列表，并保证一一对应\n",
    "    \n",
    "    参数\n",
    "    ----------\n",
    "    titles : list\n",
    "        存放某一类别下所有文章的标题\n",
    "    descriptions : list\n",
    "        存放某一类别下所有文章的描述\n",
    "    contents : list\n",
    "        存放某一类别下所有文章的内容\n",
    "        \n",
    "    返回\n",
    "    ----------\n",
    "    combined : list\n",
    "    文章的标题、描述和内容合并后的列表\n",
    "    '''\n",
    "    combined = [m+str(n)+k for m,n,k in zip(titles,descriptions,contents)]\n",
    "    \n",
    "    return combined\n",
    "\n",
    "def preprocess_categories(dir_path, tar_path, stopwords,days):\n",
    "    '''\n",
    "    用单线程对原始数据进行 分词、去除停用词、去重后 储存在本地 \n",
    "    注意！ 针对真实数据集的预处理\n",
    "    \n",
    "    参数\n",
    "    ----------\n",
    "    dir_path : string\n",
    "        数据集所在的路径 e.g. \"/Users/cyy7645/Desktop/contents/*.txt\"\n",
    "    tar_path : string\n",
    "        预处理完成后数据所存储的路径 e.g. \"/divided_context_no_sim/\"\n",
    "        \n",
    "    返回\n",
    "    ----------\n",
    "    无\n",
    "    '''\n",
    "    data_paths = glob.glob(dir_path)\n",
    "    for path in data_paths:\n",
    "        times, titles, descriptions, contents = get_contents_from_csv(path,days)\n",
    "        combined = combine_title_desc_content(titles, descriptions, contents)\n",
    "        combined = divide_words(combined,stopwords)\n",
    "        clean_df = pd.DataFrame({'create_time': times,'context': combined})\n",
    "        clean_df = clean_df[['create_time', 'context']]\n",
    "        write_path = '/'.join(path.split('/')[:-2])+ tar_path + \\\n",
    "                path.split('/')[-1]\n",
    "        clean_df.to_csv(write_path, index = False, encoding='utf_8_sig')\n",
    "        with open(write_path, \"w\") as write_file:\n",
    "            # 把分词好的文章写回到文件中\n",
    "            write_file.write(articles)\n",
    "            print(\"write content to \",path.split('/')[-1].split('.')[0],\"done\")\n",
    "           \n",
    "        \n",
    "def store_items_id():\n",
    "    '''\n",
    "    储存 get_dict_and_list_of_items 函数的返回，因为该函数预处理时间较久，方便下次直接读取\n",
    "    储存items_str, items_classes, ids_items, ids_classes, label_classes到本地\n",
    "    '''\n",
    "    with open('/Users/cyy7645/Documents/internship/data/stored_data/items_str', 'wb') as f:\n",
    "        dill.dump(items_str, f)\n",
    "    with open('/Users/cyy7645/Documents/internship/data/stored_data/items_classes', 'wb') as f:\n",
    "        dill.dump(items_classes, f)\n",
    "    with open('/Users/cyy7645/Documents/internship/data/stored_data/ids_items', 'wb') as f:\n",
    "        dill.dump(ids_items, f)\n",
    "    with open('/Users/cyy7645/Documents/internship/data/stored_data/ids_classes', 'wb') as f:\n",
    "        dill.dump(ids_classes, f)\n",
    "    with open('/Users/cyy7645/Documents/internship/data/stored_data/label_classes', 'wb') as f:\n",
    "        dill.dump(label_classes, f)\n",
    "\n",
    "\n",
    "def restore_items_id():\n",
    "    '''\n",
    "    加载数据 items_str, articles_classes, ids_articles, ids_classes, label_classes\n",
    "    '''\n",
    "    with open('/Users/cyy7645/Documents/internship/data/stored_data/items_str', 'rb') as f:\n",
    "        items_str = dill.load(f)\n",
    "    with open('/Users/cyy7645/Documents/internship/data/stored_data/items_classes', 'rb') as f:\n",
    "        items_classes = dill.load(f)\n",
    "    with open('/Users/cyy7645/Documents/internship/data/stored_data/ids_items', 'rb') as f:\n",
    "        ids_items = dill.load(f)\n",
    "    with open('/Users/cyy7645/Documents/internship/data/stored_data/ids_classes', 'rb') as f:\n",
    "        ids_classes = dill.load(f)\n",
    "    with open('/Users/cyy7645/Documents/internship/data/stored_data/label_classes', 'rb') as f:\n",
    "        label_classes = dill.load(f)\n",
    "    return items_str, items_classes, ids_items, ids_classes, label_classes\n",
    "        \n",
    "\n",
    "def store_articles_id():\n",
    "    '''\n",
    "    储存 get_dict_and_list_of_articles 函数的返回，因为该函数预处理时间较九，方便下次直接读取\n",
    "    储存articles_str, articles_classes, ids_articles, ids_classes_articles 到本地\n",
    "    '''\n",
    "    with open('/Users/cyy7645/Documents/internship/data/stored_data/articles_str', 'wb') as f:\n",
    "        dill.dump(articles_str, f)\n",
    "    with open('/Users/cyy7645/Documents/internship/data/stored_data/articles_classes', 'wb') as f:\n",
    "        dill.dump(articles_classes, f)\n",
    "    with open('/Users/cyy7645/Documents/internship/data/stored_data/ids_articles', 'wb') as f:\n",
    "        dill.dump(ids_articles, f)\n",
    "    with open('/Users/cyy7645/Documents/internship/data/stored_data/ids_classes_articles', 'wb') as f:\n",
    "        dill.dump(ids_classes_articles, f)\n",
    "    with open('/Users/cyy7645/Documents/internship/data/stored_data/label_classes', 'wb') as f:\n",
    "        dill.dump(label_classes, f)   \n",
    "        \n",
    "\n",
    "\n",
    "def restore_articles_id():\n",
    "    '''\n",
    "    加载数据 articles_str, articles_classes, ids_articles, ids_classes_articles\n",
    "    '''\n",
    "    with open('/Users/cyy7645/Documents/internship/data/stored_data/articles_str', 'rb') as f:\n",
    "        articles_str = dill.load(f)\n",
    "    with open('/Users/cyy7645/Documents/internship/data/stored_data/articles_classes', 'rb') as f:\n",
    "        articles_classes = dill.load(f)\n",
    "    with open('/Users/cyy7645/Documents/internship/data/stored_data/ids_articles', 'rb') as f:\n",
    "        ids_articles = dill.load(f)\n",
    "    with open('/Users/cyy7645/Documents/internship/data/stored_data/ids_classes_articles', 'rb') as f:\n",
    "        ids_classes_articles = dill.load(f)\n",
    "    with open('/Users/cyy7645/Documents/internship/data/stored_data/label_classes', 'rb') as f:\n",
    "        label_classes = dill.load(f)    \n",
    "        \n",
    "    return articles_str, articles_classes, ids_articles, ids_classes_articles, label_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 对原始数据分词，存入本地，读取分词结果，训练分类器，得到分类器准确率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "调用preprocess_categories函数对真实数据集预处理，若数据集已经分好词，直接加载即可。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# stop_filepath = \"/Users/cyy7645/Documents/internship/article_recom/chinese_stop_words.txt\"\n",
    "# stop_words = load_stopwords(stop_filepath)            \n",
    "# dir_path = '/Users/cyy7645/Documents/internship/data/news/*.csv'\n",
    "# tar_path = \"/cleared_news/\"\n",
    "# preprocess_categories(data_paths, tar_path, stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对预处理完成的数据集，可以采用对文章使用TF-IDF，也可以针对文章对应的关键词使用TD-IDF，同时实验发现，第二种方法在实际表现中效果更好。以下采用第二种方法为例。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "若items_str, items_classes, ids_items, ids_classes若已经储存在本地，直接使用函数 restore_items_id 加载即可。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/cyy7645/Documents/internship/data/stored_data/items_str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-bdd0055a914b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# items_str, items_classes, ids_items, ids_classes, label_classes = get_dict_and_list_of_items(path)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mitems_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitems_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids_items\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrestore_items_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;31m# 长度应一致\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids_items\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-b844db22f07a>\u001b[0m in \u001b[0;36mrestore_items_id\u001b[0;34m()\u001b[0m\n\u001b[1;32m    492\u001b[0m     \u001b[0m加载数据\u001b[0m \u001b[0mitems_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marticles_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids_articles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m     '''\n\u001b[0;32m--> 494\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/Users/cyy7645/Documents/internship/data/stored_data/items_str'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    495\u001b[0m         \u001b[0mitems_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdill\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/Users/cyy7645/Documents/internship/data/stored_data/items_classes'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/cyy7645/Documents/internship/data/stored_data/items_str'"
     ]
    }
   ],
   "source": [
    "# 本地储存keywords\n",
    "path = '/Users/cyy7645/Documents/internship/data/cleared_news/*.txt'\n",
    "\n",
    "# items_str, items_classes, ids_items, ids_classes, label_classes = get_dict_and_list_of_items(path)\n",
    "\n",
    "items_str, items_classes, ids_items, ids_classes, label_classes = restore_items_id()\n",
    "# 长度应一致\n",
    "print(len(items_str), len(items_classes),len(ids_items), len(ids_classes))\n",
    "print('标签种类数：',len(label_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_str, items_classes = shuffle_list(items_str, items_classes)\n",
    "# 打乱后的list 长度不变\n",
    "print(\"shuffle之前：\",len(items_str), \"shuffle之后：\",len(items_classes))\n",
    "# 输出前10篇文章所属的类，检查是否乱序\n",
    "print(\"shffle之后前10篇文章的类别\",items_classes[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集的TF-IDF大小： (587996, 559293) 测试集的TF-IDF大小： (65332, 559293)\n",
      "<class 'sklearn.feature_extraction.text.TfidfVectorizer'>\n"
     ]
    }
   ],
   "source": [
    "train_vectors, test_vectors, articles_classes_test_np, articles_classes_train_np, \\\n",
    "    vectorizer, vectors =  split_vectors_and_classes(items_str, items_classes)\n",
    "# TF-IDF产生的 matrix\n",
    "# print(vectors.shape)\n",
    "# print(vectors.nnz / float(vectors.shape[0]))\n",
    "print(\"训练集的TF-IDF大小：\",train_vectors.shape, \"测试集的TF-IDF大小：\",test_vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1-score:  0.395586525383\n",
      "accuracy:  0.719065695218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cyy7645/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# 使用朴素贝叶斯检查分类的准确率\n",
    "clf = MultinomialNB(alpha=.01)\n",
    "clf.fit(train_vectors, articles_classes_train_np)\n",
    "pred = clf.predict(test_vectors)\n",
    "f1 = metrics.f1_score(articles_classes_test_np, pred, average='macro')\n",
    "acc = metrics.accuracy_score(articles_classes_test_np, pred)\n",
    "print(\"f1-score: \", f1)\n",
    "print(\"accuracy: \",acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 对新文章进行预处理函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 得到一篇新的文章，在其所属的类中根据相似度指标找出相似度最高的一篇"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_data_to_keywords(path, stopwords, days):\n",
    "    '''\n",
    "    对原始数据集进行 分词、去除停用词、去重，\n",
    "    然后生成储存文章关键字和其所属类别的 list和dict\n",
    "    \n",
    "    参数\n",
    "    ----------\n",
    "    path : string\n",
    "        预处理完成的数据所在的路径，e.g. path = \n",
    "                    '/Users/cyy7645/Documents/internship/data/csv_news/*.csv'\n",
    "    days : int 默认1000\n",
    "        从今天开始往前n天的数据作为相似度比较的数据\n",
    "    \n",
    "    返回\n",
    "    ----------\n",
    "    items_str : list\n",
    "        储存整个数据集的文章对应的关键字\n",
    "    items_classes : list\n",
    "        储存整个数据集的文章对应的关键字所对应的类[int]\n",
    "    ids_items : dict\n",
    "        储存文章对应的关键字和其类别一一对应的字典\n",
    "        value为文章对应的关键字，key为文章对应的类 int\n",
    "    ids_classes : dict\n",
    "        储存文章id和其所属类别的字典\n",
    "        key为文章的id，value为该文章所属的类别\n",
    "    label_classes : dict\n",
    "        储存标签和id的字典\n",
    "        key为标签，value为id\n",
    "    \n",
    "    '''\n",
    "    divided_paths = glob.glob(path)\n",
    "    items_str = []\n",
    "    ids_items = {}\n",
    "    items_classes = []\n",
    "    ids_classes = {}\n",
    "    label_classes = {}\n",
    "    count_articles = 1\n",
    "    article_class = 1\n",
    "    for path in divided_paths:\n",
    "        print(\"start process \",path.split('/')[-1],\"...\")\n",
    "        ids, times, titles, descriptions, contents = get_contents_from_csv(path, days)\n",
    "        combined = combine_title_desc_content(titles, descriptions, contents)\n",
    "        combined = divide_words(combined,stopwords)\n",
    "        df = pd.DataFrame({'id':ids,'create_time': times,'context': combined})\n",
    "        df = df[['id','create_time', 'context']]\n",
    "        \n",
    "\n",
    "        print(df.shape)\n",
    "        articles_dict = {}\n",
    "        label = path.split('/')[-1].split('.')[0]\n",
    "        for index, row in df.iterrows():\n",
    "            context = row['context']\n",
    "            id_num = row['id']\n",
    "            items,weights = extract_keywords(context, topK=30, \n",
    "                                             withWeight=True, allowPOS=())\n",
    "            items_str.append(items)\n",
    "            ids_items[id_num] = items\n",
    "            items_classes.append(article_class)\n",
    "            ids_classes[id_num] = article_class\n",
    "        label_classes[label] = article_class\n",
    "        article_class += 1\n",
    "        print(\"preprocess \",path.split('/')[-1],\"done ...\")\n",
    "    return items_str, items_classes, ids_items, ids_classes, label_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_new_article(doc_path, stop_filepath, vectorizer, items_classes, vectors,items_str, ids_items,\n",
    "                           cut_all = False, cut_for_search = False,  n_docs = 3, label = None):\n",
    "    '''\n",
    "    对新输入的文章进行预处理：分词，去除停用词，提取关键词，得到TF-IDF值和数据库中的所有文章进行相似度比较，\n",
    "                          返回相似度最高的n篇文章，并把新文章写入到一个文件（因为对此后新输入的文章需要同\n",
    "                          此篇文章进行相似度比较）\n",
    "    \n",
    "    参数\n",
    "    ----------\n",
    "    path : string\n",
    "        新文章所在的路径\n",
    "    stop_filepath : string\n",
    "        停用词文件所在的路径\n",
    "    cut_all : bool 默认 False\n",
    "        当值为True时，使用全模式分词\n",
    "    cut_for_search : bool 默认 False\n",
    "        当值为True时，使用搜索引擎模式\n",
    "    vectorizer: class\n",
    "        文本特征提取器\n",
    "    clf : object\n",
    "        分类器\n",
    "    n_docs : int 默认为3\n",
    "        返回几篇最相近的文章\n",
    "    label : string 默认 None\n",
    "        新文章所属的类别\n",
    "    \n",
    "    返回\n",
    "    ----------\n",
    "    most_simi_doc : list\n",
    "        与输入文章最相似的文章列表\n",
    "    '''\n",
    "    keys = []\n",
    "    with open (doc_path, 'r') as p:\n",
    "        new_doc = p.read().split(\"\\n\")\n",
    "        new_doc = ' '.join(new_doc)\n",
    "    keywords,weights = extract_keywords(new_doc, topK=30, withWeight=True, allowPOS=())\n",
    "    new_doc = ' '.join(keywords)\n",
    "    new_doc = [new_doc]\n",
    "    \n",
    " \n",
    "    # 得到这篇文章的tf-idf值\n",
    "    vectors_new_doc = vectorizer.transform(new_doc)\n",
    "    \n",
    "    \n",
    "    # 计算新文章与数据库中文章的相似度\n",
    "    cosine_similarities = linear_kernel(vectors[:], vectors_new_doc).flatten()\n",
    "    \n",
    "    # 根据相关性找到最相似的文章编号\n",
    "    related_docs_indices = cosine_similarities.argsort()[:-100:-1]\n",
    "#     print(related_docs_indices)\n",
    "#     print(sorted(cosine_similarities.tolist())[-5:])\n",
    "    most_simi_doc = []\n",
    "    while n_docs > 0:\n",
    "        simi_doc = ' '.join(items_str[int(related_docs_indices[-n_docs])])\n",
    "        most_simi_doc.append(simi_doc)\n",
    "        keys.append(list(ids_items.keys())[list(ids_items.values()).index(items_str[int(related_docs_indices[-n_docs])])])\n",
    "        n_docs -= 1\n",
    "    reversed(most_simi_doc)\n",
    "    \n",
    "#     dicxx = {'a':'001', 'b':'002'}\n",
    "#     list(ids_items.keys())[list(ids_items.values()).index(\"001\")]\n",
    "    \n",
    "#     # 把这篇新文章的关键词写入到文件中\n",
    "#     with open(\"/Users/cyy7645/Desktop/new_docs.txt\",\"r+\") as f:\n",
    "#         f.read()\n",
    "#         f.write('\\n')\n",
    "#         f.write(new_doc)\n",
    "        \n",
    "    return most_simi_doc, keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['边缘 联网 计算 云端 设备 数据 终端 智能 海量 网络 信息 场景 技术 工业 智慧 智能化 计算能力 存储 协同 落地', '超算 中国 超级计算机 任新民 巨星 抗洪 董万瑞 神威 之光 大堤 太湖 文章 世界 领域 42 去世 日本 两弹一星 歌星 压倒性', '招收 军队院校 女生 院校 2017 招生 大学 14 普通高中 毕业生 26 1.2 416 信息工程 计划 工程 万名 国防科技大学 20 各省市']\n",
      "[26]\n"
     ]
    }
   ],
   "source": [
    "# 对相似文章推荐进行测试 \n",
    "path = \"./new_doc.txt\"\n",
    "most_simi_doc = preprocess_new_article(path, stop_filepath, vectorizer, items_classes, clf, vectors, label_classes, \n",
    "                           cut_all = False, cut_for_search = False,  n_docs = 3, label = None)\n",
    "print(most_simi_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 5. 储存加载中间变量，方便func2从本地调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def store_tmp(store_path,vectorizer, vectors, label_classes, items_classes, items_str,ids_items):\n",
    "    '''\n",
    "    储存func1中间文件的函数\n",
    "    '''\n",
    "    with open(store_path + 'vectorizer', 'wb') as f:\n",
    "        dill.dump(vectorizer, f)\n",
    "    with open(store_path + 'vectors', 'wb') as f:\n",
    "        dill.dump(vectors, f)\n",
    "    with open(store_path + 'label_classes', 'wb') as f:\n",
    "        dill.dump(label_classes, f)\n",
    "    with open(store_path + 'items_classes', 'wb') as f:\n",
    "        dill.dump(items_classes, f)\n",
    "    with open(store_path + 'items_str', 'wb') as f:\n",
    "        dill.dump(items_str, f)\n",
    "    with open(store_path + 'ids_items', 'wb') as f:\n",
    "        dill.dump(ids_items, f)\n",
    "        \n",
    "def restore_tmp(restore_path):\n",
    "    '''\n",
    "    加载func1中间文件的函数\n",
    "    '''\n",
    "    with open(store_path + 'vectorizer', 'rb') as f:\n",
    "        vectorizer = dill.load(f)\n",
    "    with open(store_path + 'vectors', 'rb') as f:\n",
    "        vectors = dill.load(f)\n",
    "    with open(store_path + 'label_classes', 'rb') as f:\n",
    "        label_classes = dill.load(f)\n",
    "    with open(store_path + 'items_classes', 'rb') as f:\n",
    "        items_classes = dill.load(f)\n",
    "    with open(store_path + 'items_str', 'rb') as f:\n",
    "        items_str = dill.load(f)\n",
    "    with open(store_path + 'ids_items', 'rb') as f:\n",
    "        ids_items = dill.load(f)\n",
    "    return vectorizer, vectors, label_classes, items_classes, items_str,ids_items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. 对外接口函数   每隔一段时间运行func1, 为新输入文章找相似文章运行func2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "以下两个函数是对上述函数的封装，作为对外提供的api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 函数一：每隔一段时间运行\n",
    "\n",
    "def func1(path, stop_filepath, store_path, days):\n",
    "    '''\n",
    "    对原始数据集进行预处理，包括分词、去除停用词、生成TF-IDF表等，最后将结果存在本地，方便func2调用\n",
    "    \n",
    "    参数\n",
    "    ----------------\n",
    "    path : string\n",
    "        原始数据集路径\n",
    "    stop_filepath : string\n",
    "        停用词词表的存放路径\n",
    "    store_path : string\n",
    "        存放结果的路径\n",
    "        \n",
    "    返回值\n",
    "    ----------------\n",
    "    无\n",
    "    '''\n",
    "    # 若 stop_filepath文件夹不存在，创建\n",
    "    if not os.path.exists(store_path):\n",
    "        os.makedirs(store_path)\n",
    "    # 加载停用词表\n",
    "    stopwords = load_stopwords(stop_filepath)\n",
    "    items_str, items_classes, ids_items, ids_classes, label_classes = preprocess_data_to_keywords(path, stopwords, days)\n",
    "    train_vectors, test_vectors, articles_classes_test_np, articles_classes_train_np, \\\n",
    "    vectorizer, vectors =  split_vectors_and_classes(items_str, items_classes)\n",
    "    store_tmp(store_path,vectorizer, vectors, label_classes, items_classes, items_str,ids_items)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start process  就业.csv ...\n",
      "(1, 3)\n",
      "preprocess  就业.csv done ...\n",
      "start process  移动应用.csv ...\n",
      "(19, 3)\n",
      "preprocess  移动应用.csv done ...\n",
      "start process  体育.csv ...\n",
      "(2198, 3)\n",
      "preprocess  体育.csv done ...\n",
      "start process  人工智能.csv ...\n",
      "(2, 3)\n",
      "preprocess  人工智能.csv done ...\n",
      "start process  GRE.csv ...\n",
      "(2, 3)\n",
      "preprocess  GRE.csv done ...\n",
      "start process  酵素.csv ...\n",
      "(9, 3)\n",
      "preprocess  酵素.csv done ...\n",
      "start process  社会.csv ...\n",
      "(1126, 3)\n",
      "preprocess  社会.csv done ...\n",
      "start process  孕婴.csv ...\n",
      "(8, 3)\n",
      "preprocess  孕婴.csv done ...\n",
      "start process  保险.csv ...\n",
      "(203, 3)\n",
      "preprocess  保险.csv done ...\n",
      "start process  美容整形.csv ...\n",
      "(2, 3)\n",
      "preprocess  美容整形.csv done ...\n",
      "start process  明星.csv ...\n",
      "(153, 3)\n",
      "preprocess  明星.csv done ...\n",
      "start process  海归就业.csv ...\n",
      "(1, 3)\n",
      "preprocess  海归就业.csv done ...\n",
      "start process  出国.csv ...\n",
      "(2, 3)\n",
      "preprocess  出国.csv done ...\n",
      "start process  时尚.csv ...\n",
      "(394, 3)\n",
      "preprocess  时尚.csv done ...\n",
      "start process  移民.csv ...\n",
      "(3, 3)\n",
      "preprocess  移民.csv done ...\n",
      "start process  文化.csv ...\n",
      "(473, 3)\n",
      "preprocess  文化.csv done ...\n",
      "start process  羽毛球.csv ...\n",
      "(3, 3)\n",
      "preprocess  羽毛球.csv done ...\n",
      "start process  GMAT.csv ...\n",
      "(3, 3)\n",
      "preprocess  GMAT.csv done ...\n",
      "start process  足球.csv ...\n",
      "(381, 3)\n",
      "preprocess  足球.csv done ...\n",
      "start process  农资行情.csv ...\n",
      "(243, 3)\n",
      "preprocess  农资行情.csv done ...\n",
      "start process  投资.csv ...\n",
      "(7, 3)\n",
      "preprocess  投资.csv done ...\n",
      "start process  互联网+.csv ...\n",
      "(96, 3)\n",
      "preprocess  互联网+.csv done ...\n",
      "start process  健康.csv ...\n",
      "(13, 3)\n",
      "preprocess  健康.csv done ...\n",
      "start process  财经.csv ...\n",
      "(4392, 3)\n",
      "preprocess  财经.csv done ...\n",
      "start process  大数据.csv ...\n",
      "(5, 3)\n",
      "preprocess  大数据.csv done ...\n",
      "start process  国际.csv ...\n",
      "(1844, 3)\n",
      "preprocess  国际.csv done ...\n",
      "start process  通信.csv ...\n",
      "(45, 3)\n",
      "preprocess  通信.csv done ...\n",
      "start process  职场.csv ...\n",
      "(8, 3)\n",
      "preprocess  职场.csv done ...\n",
      "start process  游戏.csv ...\n",
      "(955, 3)\n",
      "preprocess  游戏.csv done ...\n",
      "start process  农业气象.csv ...\n",
      "(32, 3)\n",
      "preprocess  农业气象.csv done ...\n",
      "start process  影视.csv ...\n",
      "(93, 3)\n",
      "preprocess  影视.csv done ...\n",
      "start process  生活.csv ...\n",
      "(215, 3)\n",
      "preprocess  生活.csv done ...\n",
      "start process  食品安全.csv ...\n",
      "(6, 3)\n",
      "preprocess  食品安全.csv done ...\n",
      "start process  养生.csv ...\n",
      "(793, 3)\n",
      "preprocess  养生.csv done ...\n",
      "start process  公司.csv ...\n",
      "(1, 3)\n",
      "preprocess  公司.csv done ...\n",
      "start process  华人.csv ...\n",
      "(327, 3)\n",
      "preprocess  华人.csv done ...\n",
      "start process  AI前沿.csv ...\n",
      "(44, 3)\n",
      "preprocess  AI前沿.csv done ...\n",
      "start process  医药.csv ...\n",
      "(300, 3)\n",
      "preprocess  医药.csv done ...\n",
      "start process  英语.csv ...\n",
      "(28, 3)\n",
      "preprocess  英语.csv done ...\n",
      "start process  网球.csv ...\n",
      "(1, 3)\n",
      "preprocess  网球.csv done ...\n",
      "start process  签证.csv ...\n",
      "(1, 3)\n",
      "preprocess  签证.csv done ...\n",
      "start process  智能.csv ...\n",
      "(79, 3)\n",
      "preprocess  智能.csv done ...\n",
      "start process  雅思.csv ...\n",
      "(3, 3)\n",
      "preprocess  雅思.csv done ...\n",
      "start process  光伏.csv ...\n",
      "(156, 3)\n",
      "preprocess  光伏.csv done ...\n",
      "start process  时政.csv ...\n",
      "(324, 3)\n",
      "preprocess  时政.csv done ...\n",
      "start process  旅游.csv ...\n",
      "(863, 3)\n",
      "preprocess  旅游.csv done ...\n",
      "start process  家电.csv ...\n",
      "(178, 3)\n",
      "preprocess  家电.csv done ...\n",
      "start process  星座.csv ...\n",
      "(22, 3)\n",
      "preprocess  星座.csv done ...\n",
      "start process  华侨.csv ...\n",
      "(10, 3)\n",
      "preprocess  华侨.csv done ...\n",
      "start process  农业生产.csv ...\n",
      "(1, 3)\n",
      "preprocess  农业生产.csv done ...\n",
      "start process  购物.csv ...\n",
      "(1, 3)\n",
      "preprocess  购物.csv done ...\n",
      "start process  疾病.csv ...\n",
      "(91, 3)\n",
      "preprocess  疾病.csv done ...\n",
      "start process  教育.csv ...\n",
      "(567, 3)\n",
      "preprocess  教育.csv done ...\n",
      "start process  科技.csv ...\n",
      "(1240, 3)\n",
      "preprocess  科技.csv done ...\n",
      "start process  化肥.csv ...\n",
      "(156, 3)\n",
      "preprocess  化肥.csv done ...\n",
      "start process  留学.csv ...\n",
      "(1969, 3)\n",
      "preprocess  留学.csv done ...\n",
      "start process  区块链.csv ...\n",
      "(2, 3)\n",
      "preprocess  区块链.csv done ...\n",
      "start process  科学.csv ...\n",
      "(17, 3)\n",
      "preprocess  科学.csv done ...\n",
      "start process  娱乐.csv ...\n",
      "(2651, 3)\n",
      "preprocess  娱乐.csv done ...\n",
      "start process  汽车.csv ...\n",
      "(2, 3)\n",
      "preprocess  汽车.csv done ...\n",
      "start process  Fintech.csv ...\n",
      "(3, 3)\n",
      "preprocess  Fintech.csv done ...\n",
      "start process  托福.csv ...\n",
      "(5, 3)\n",
      "preprocess  托福.csv done ...\n",
      "start process  SAT.csv ...\n",
      "(29, 3)\n",
      "preprocess  SAT.csv done ...\n",
      "start process  乒乓球.csv ...\n",
      "(1, 3)\n",
      "preprocess  乒乓球.csv done ...\n",
      "start process  艺术.csv ...\n",
      "(640, 3)\n",
      "preprocess  艺术.csv done ...\n",
      "start process  综合.csv ...\n",
      "(20, 3)\n",
      "preprocess  综合.csv done ...\n",
      "start process  互联网.csv ...\n",
      "(280, 3)\n",
      "preprocess  互联网.csv done ...\n",
      "start process  国家大剧院.csv ...\n",
      "(46, 3)\n",
      "preprocess  国家大剧院.csv done ...\n",
      "start process  股票.csv ...\n",
      "(226, 3)\n",
      "preprocess  股票.csv done ...\n",
      "start process  军事.csv ...\n",
      "(766, 3)\n",
      "preprocess  军事.csv done ...\n",
      "start process  历史.csv ...\n",
      "(1474, 3)\n",
      "preprocess  历史.csv done ...\n",
      "start process  陶瓷.csv ...\n",
      "(45, 3)\n",
      "preprocess  陶瓷.csv done ...\n",
      "start process  新闻.csv ...\n",
      "(662, 3)\n",
      "preprocess  新闻.csv done ...\n",
      "start process  国内.csv ...\n",
      "(2219, 3)\n",
      "preprocess  国内.csv done ...\n",
      "start process  美食.csv ...\n",
      "(45, 3)\n",
      "preprocess  美食.csv done ...\n"
     ]
    }
   ],
   "source": [
    "# 运行 func1 函数\n",
    "store_path = './tmp/'\n",
    "path = \"./news/*.csv\"\n",
    "func1(path,stop_filepath,store_path, days=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def func2(path, stop_filepath, write_path,restore_path):\n",
    "    '''\n",
    "    读取func1的结果，根据新文章所在的地方，把相似文章的id写入到文件中\n",
    "    \n",
    "    参数\n",
    "    ------------------\n",
    "    path : string\n",
    "        待比较相似度的文章所在的地址\n",
    "    stop_filepath : string\n",
    "        停用词词表的存放路径\n",
    "    write_path : string\n",
    "        相似文章id结果存放路径\n",
    "    restore_path : string\n",
    "        func1结果存放的路径\n",
    "        \n",
    "    返回值\n",
    "    ------------------\n",
    "    most_simi_doc : list\n",
    "        相似度最高的文章列表\n",
    "    keys : list\n",
    "        相似度最高的文章对应id的列表\n",
    "    '''\n",
    "    \n",
    "    vectorizer, vectors, label_classes, items_classes, items_str,ids_items = restore_tmp(restore_path)\n",
    "    \n",
    "    most_simi_doc, keys = preprocess_new_article(path, stop_filepath, vectorizer, items_classes, vectors, items_str,ids_items)\n",
    "    f = open(write_path, 'w')\n",
    "    for i in range(len(keys)):\n",
    "        if i < len(keys)-1:\n",
    "            f.write(str(keys[i]))\n",
    "            f.write(',')\n",
    "        else:\n",
    "            f.write(str(keys[i]))\n",
    "    f.close()\n",
    "    return most_simi_doc,keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "[2572341, 2592461, 2592362]\n"
     ]
    }
   ],
   "source": [
    "# 运行 func2 函数\n",
    "restore_path = './tmp/'\n",
    "doc_path = \"./new_doc.txt\"\n",
    "write_path = \"./write_keys.txt\"\n",
    "most_simi_doc,keys  = func2(doc_path, stop_filepath, write_path,restore_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['并购 海外 国有企业 企业 中国 4.6 2016 国企 境外 投资 监管 2015 中企 全球 收购 总金额 2010 1800 先正达 唱主角', '众安 2016 用户 保费 承保 场景 保单 科技 金融 估值 生态 车险 生态圈 IPO 航旅 健康 消费 亿元 互联网 4.92', '融创 万达 2016 金科 亿元 酒店 孙宏斌 收购 乐视 王健林 2017 项目 视网 nbsp 顺驰 文旅 资金 中国 公司 并购']\n",
      "[2572341, 2592461, 2592362]\n"
     ]
    }
   ],
   "source": [
    "print(most_simi_doc)\n",
    "print(keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
